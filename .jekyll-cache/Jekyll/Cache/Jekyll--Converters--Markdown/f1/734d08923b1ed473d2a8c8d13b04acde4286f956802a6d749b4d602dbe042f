I"m<p>In my study of Deep Learning, there have been a lot of times that I’ve created something cool but had no way to show it to anyone. I found it the hard way that not many people believe in a complex system like Speech Recognition looking at the loss graph and the output of some test data.</p>

<p>The problems I faced:
I built a great AI model, finetuning it for weeks. I tried to show it to other people but I had to.</p>
<ul>
  <li>Load google colab</li>
  <li>Connect it to google drive</li>
  <li>Copy and load the model</li>
  <li>Wait…</li>
</ul>

<p>That is enough to kill a man’s enthusiasm .</p>

<p>I copy the colab code into a server, build a flask API around it and try to run it. It now shows the kind of tensorflow errors that I’d never seen before. Running tensorflow graphs and trying to build an API around it in the same program doesn’t work that easily.</p>

<p>Upon further research I found about tensorflow serving.  It serves a tensorflow model as a REST server. 
And It works pretty well.</p>

<p>For simple cases, tf.keras.save_model() works fine but not if you have weirder things going inside your model.</p>

<p>Prerequisites: Tensorflow, Docker</p>

<p>You need not know much about docker but a good bit about tensorflow.</p>

<p>Step 1:
The first step is to save the model.</p>
<ul>
  <li>Initially save the model using tf.keras.save_model() and if it doesn’t work.</li>
  <li>Study about getting graphs from your tensorflow model. It varies according to use-cases but the <a href="https://www.tensorflow.org/guide/function">documentation</a> is pretty good.<br />
A Code example:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">callable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">call</span><span class="p">)</span>
  <span class="n">concrete_function</span> <span class="o">=</span> <span class="nb">callable</span><span class="p">.</span><span class="n">get_concrete_function</span><span class="p">(......)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="err">“</span><span class="n">filename</span><span class="err">”</span><span class="p">,</span> <span class="n">signatures</span><span class="o">=</span><span class="n">concrete_function</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>(That is how I once served a BERT model)</p>
  </li>
</ul>

<p>Now that the model is saved, next is to serve it. 
Step 2:</p>
<ul>
  <li>Tensorflow serving
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker pull tensorflow/serving
</code></pre></div>    </div>
  </li>
  <li>Run it:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">-p</span> 8501:8501 <span class="se">\</span>
<span class="nt">--mount</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bind</span>,source<span class="o">=</span>/path/to/my_model/,target<span class="o">=</span>/models/my_model <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">MODEL_NAME</span><span class="o">=</span>my_model <span class="nt">-t</span> tensorflow/serving
</code></pre></div>    </div>
  </li>
</ul>

<p>Is it done?
Well no, You’ve only served a model that outputs logits or probability distribution of something. That, most people are not going to get. So, you’d need to build a flask app around it. (You could use something other than flask but you might need to use tensorflow again for post-processing of those logits). Now, the flask app just needs to send a HTTP request to port 8501.</p>

<p><em>If it’s a cats/dogs classification, don’t directly send the picture of a cat. Instead, read the <a href="https://www.tensorflow.org/tfx/tutorials/serving/rest_simple">documentation.</a></em></p>

<p>Just like that you have a <a href="http://165.22.177.37:7000/chatbotfront">chatbot</a></p>

<p>References:</p>
<ol>
  <li>https://www.tensorflow.org/tfx/guide/serving</li>
  <li>https://www.tensorflow.org/tfx/serving/docker</li>
  <li>https://www.tensorflow.org/tfx/tutorials/serving/rest_simple</li>
  <li>https://www.tensorflow.org/guide/function</li>
</ol>

<p><em>I’m currently working on Emotion Recognition with Landmark Detection and hopefully I’ll have enough motivation to deploy it and release the code accompanying this post.</em></p>
:ET