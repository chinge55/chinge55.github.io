I"»<p>In my study of Deep Learning, there have been a lot of times that I‚Äôve created something cool but had no way to show it to anyone. I found it the hard way that not many people believe in a complex system like Speech Recognition looking at the loss graph and output of some test data.</p>

<p>The problems I faced:
I built a great AI model, finetuning it for weeks. I tried to show it to other people but I had to.</p>
<ul>
  <li>Load google colab</li>
  <li>Connect it to google drive</li>
  <li>Copy and load the model</li>
  <li>Wait‚Ä¶
That is enough to kill a man‚Äôs enthusiasm .</li>
</ul>

<p>I copy the colab code into a server, build a flask API around it and try to run it. It now shows the kind of tensorflow errors that I‚Äôd never seen before. Running tensorflow graphs and trying to build an API around it in the same program doesn‚Äôt work that easily.</p>

<p>Upon further research I found about tensorflow serving.  It serves the Deep Learning model as a REST server. 
And It works pretty well.</p>

<p>For simple cases, tf.keras.save_model() works fine but not if you have weirder things going inside your model.</p>

<p>Prerequisites: Tensorflow, Docker
You need not know much about docker but a good bit about tensorflow.</p>

<p>Step 1:
The first step is to save the model.</p>
<ul>
  <li>Initially save the model using tf.keras.save_model() and if it doesn‚Äôt work.</li>
  <li>Study about getting graphs from your tensorflow model. It varies according to use-cases but the documentation is pretty good. https://www.tensorflow.org/guide/function<br />
A Code example:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">callable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">call</span><span class="p">)</span>
  <span class="n">concrete_function</span> <span class="o">=</span> <span class="nb">callable</span><span class="p">.</span><span class="n">get_concrete_function</span><span class="p">(......)</span>
  <span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="err">‚Äú</span><span class="n">filename</span><span class="err">‚Äù</span><span class="p">,</span> <span class="n">signatures</span><span class="o">=</span><span class="n">concrete_function</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>(That is how I once served a BERT model)</p>
  </li>
</ul>

<p>Now that the model is saved, next is to serve it. 
Step 2:</p>
<ul>
  <li>Tensorflow serving
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker pull tensorflow/serving
</code></pre></div>    </div>
  </li>
  <li>Run it:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>docker run <span class="nt">-p</span> 8501:8501 <span class="se">\</span>
<span class="nt">--mount</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bind</span>,source<span class="o">=</span>/path/to/my_model/,target<span class="o">=</span>/models/my_model <span class="se">\</span>
<span class="nt">-e</span> <span class="nv">MODEL_NAME</span><span class="o">=</span>my_model <span class="nt">-t</span> tensorflow/serving
</code></pre></div>    </div>
  </li>
</ul>
:ET